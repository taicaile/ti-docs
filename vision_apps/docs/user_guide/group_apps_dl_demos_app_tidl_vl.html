<!-- HTML header for doxygen 1.8.11-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<link rel="shortcut icon" href="favicon.png" type="image/png">    
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.14"/>
<title>Vision Apps User Guide: Visual localization Application</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<style>
.tinav {
    background: #c00;
    /* height: 41.375px; */
    height: 30px;
    }
</style>    
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
  $(document).ready(initResizable);
/* @license-end */</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
  $(document).ready(function() { init_search(); });
/* @license-end */
</script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="stylesheet.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 40px;">
  <td id="projectlogo"><a href="https://www.ti.com"><img alt="Logo" src="ti_logo.png"/></a></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">Vision Apps User Guide
   </div>
  </td>
   <td>        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <img id="MSearchSelect" src="search/mag_sel.png"
               onmouseover="return searchBox.OnSearchSelectShow()"
               onmouseout="return searchBox.OnSearchSelectHide()"
               alt=""/>
          <input type="text" id="MSearchField" value="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="search/close.png" alt=""/></a>
          </span>
        </div>
</td>
 </tr>
 </tbody>
</table>
<div class=tinav></div>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.14 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search');
/* @license-end */
</script>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(document).ready(function(){initNavTree('group_apps_dl_demos_app_tidl_vl.html','');});
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">Visual localization Application </div>  </div>
</div><!--header-->
<div class="contents">
<div class="toc"><h3>Table of Contents</h3>
<ul><li class="level1"><a href="#autotoc_md140">Introduction</a></li>
<li class="level1"><a href="#autotoc_md142">Supported platforms</a></li>
<li class="level1"><a href="#autotoc_md143">Steps to run the application on J7 EVM (Linux + RTOS mode)</a></li>
<li class="level1"><a href="#autotoc_md144">Steps to run the application on J7 EVM (QNX + RTOS mode)</a></li>
<li class="level1"><a href="#autotoc_md145">Steps to run the application on PC Linux x86_64</a></li>
<li class="level1"><a href="#autotoc_md147">Visual Localization Algorithm Overview</a></li>
<li class="level1"><a href="#autotoc_md149">Deep learning based descriptor (DKAZE)</a></li>
<li class="level1"><a href="#autotoc_md151">Dataset creation using Carla Simulator</a></li>
<li class="level1"><a href="#autotoc_md153">Localization Accuracy</a></li>
<li class="level1"><a href="#autotoc_md155">Visual Localization Data Flow</a></li>
<li class="level1"><a href="#autotoc_md157">Sparse 3D Map</a></li>
<li class="level1"><a href="#autotoc_md159">Optimized Building Blocks for your own Visual localization pipeline</a></li>
<li class="level1"><a href="#autotoc_md161">Sample Output</a></li>
</ul>
</div>
<div class="textblock"><h1><a class="anchor" id="autotoc_md140"></a>
Introduction</h1>
<p>The tasks involved in Autonomous/Automated Driving (AD) can be categorized in 4 modules of 1) perception, 2) localization 3) mapping and 4) planning and control. The localization module determines position of the vehicle i.e. 3D location and orientation and plays a critical role in safe and comfortable navigation. Localization is the process of estimating vehicle position in the paradigm of map-based navigation. When localization process uses only camera sensor it is referred to as Visual Localization. Visual localization is particularly valuable when other means of localization are unavailable e.g. GPS denied environments such as indoor locations. Also Visual Localization output can be fused with noisy Inertial Navigation System(INS) to obtain overall accurate vehicle positions. Along with self-driving cars other applications include robot navigation and augmented reality. </p><hr/>
<h1><a class="anchor" id="autotoc_md142"></a>
Supported platforms</h1>
<table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadNone">Platform  </th><th class="markdownTableHeadNone">Linux x86_64  </th><th class="markdownTableHeadNone">Linux+RTOS mode  </th><th class="markdownTableHeadNone">QNX+RTOS mode  </th><th class="markdownTableHeadNone">SoC   </th></tr>
<tr class="markdownTableBody" class="markdownTableRowOdd">
<td class="markdownTableBodyNone">Support  </td><td class="markdownTableBodyNone">YES  </td><td class="markdownTableBodyNone">YES  </td><td class="markdownTableBodyNone">YES  </td><td class="markdownTableBodyNone">J721e / J721S2 / J784S4   </td></tr>
</table>
<h1><a class="anchor" id="autotoc_md143"></a>
Steps to run the application on J7 EVM (Linux + RTOS mode)</h1>
<ol type="1">
<li>Build the application and related libraries as mentioned in <a class="el" href="BUILD_INSTRUCTIONS.html">Build Instructions</a> for Linux+RTOS mode</li>
<li>A sample "app_vl.cfg" for Visual Localization is provided under "/opt/vision_apps/" on the rootfs partition.</li>
<li>Create a folder and keep all the .yuv files used for detection, there are a few images under "/opt/vision_apps/test_data/psdkra/app_visual_localization/inputs".</li>
<li>Run the app as shown below <div class="fragment"><div class="line">/opt/vision_apps&gt;. ./vision_apps_init.sh</div><div class="line">/opt/vision_apps&gt;./run_app_tidl_vl.sh</div></div><!-- fragment --></li>
</ol>
<h1><a class="anchor" id="autotoc_md144"></a>
Steps to run the application on J7 EVM (QNX + RTOS mode)</h1>
<ol type="1">
<li>Build the application and related libraries as mentioned in <a class="el" href="BUILD_INSTRUCTIONS.html">Build Instructions</a> for QNX+RTOS mode</li>
<li>A sample "app_vl.cfg" for Visual Localization is provided under "/ti_fs/vision_apps/" on the boot partition.</li>
<li>Create a folder and keep all the .yuv files used for detection, there are a few images under "/ti_fs/vision_apps/test_data/psdkra/app_visual_localization/inputs".</li>
<li>Run the app as shown below <div class="fragment"><div class="line">/ti_fs/vision_apps&gt;. ./vision_apps_init.sh</div><div class="line">/ti_fs/vision_apps&gt;./run_app_tidl_vl.sh</div></div><!-- fragment --></li>
</ol>
<h1><a class="anchor" id="autotoc_md145"></a>
Steps to run the application on PC Linux x86_64</h1>
<ol type="1">
<li>Build the application and related libraries as mentioned in <a class="el" href="BUILD_INSTRUCTIONS.html">Build Instructions</a> for PC emulation mode</li>
<li>A sample "app_vl.cfg" for Visual Localization is provided under "${PSDKR_PATH}/vision_apps/apps/dl_demos/app_tidl_vl/config" folder.</li>
<li>Create a folder and keep all the .yuv files used for detection, there are a few images under "/opt/vision_apps/psdkra/app_visual_localization/inputs".</li>
<li>Create an output directory <div class="fragment"><div class="line">mkdir vl_out</div></div><!-- fragment --></li>
<li>Run the app as shown below <div class="fragment"><div class="line">./vx_app_tidl_vl --cfg ${PSDKR_PATH}/vision_apps/apps/dl_demos/app_tidl_vl/config/app_vl.cfg</div></div><!-- fragment --></li>
<li>The output will be written in "vl_out" folder in .yuv (NV12) format. <hr/>
</li>
</ol>
<h1><a class="anchor" id="autotoc_md147"></a>
Visual Localization Algorithm Overview</h1>
<p>The example Visual Localization (VL) scheme performs 6-Degrees of Freedom ego-localization using 3D sparse map of area of automated operation and a query image. Each point in the sparse map stores visual and location information of the scene key-point which can be used at localization time to match against the key-points in the query image. The Visual Localization process thus is built around camera pose estimation approach such as Perspective-n-Point (PnP) problem. PnP is the problem of estimating the pose of a calibrated camera given a set of n 3D points in the world and their corresponding 2D projections in the image. Considering the perspective transform as defined in equation below the problem then is to estimate the rotation and translation matrix [R|T] of the camera when provided with N numbers of world(Pw) - camera(Pc) point correspondences.</p>
<div class="image">
<img src="RT.png" alt="RT.png"/>
</div>
<p>The [R|T] matrix encodes the camera pose in 6 degrees-of-freedom (DoF) which are made up of the rotation (roll, pitch, and yaw) and 3D translation of the camera with respect to the world. The VL process thus involves detection and description of key-points points in the query image captured by the onboard camera of the vehicle, matching of the 2D image points with that of 3D landmark points from map to establish 2D-3D point correspondences, followed by camera-pose estimation using any of the SolvePnP solutions. This simplified process is depicted in the following Fig. Offline calibration of the camera sensors and vehicles frame of reference provides the extrinsic parameters that can be used to estimate the vehicle pose from the estimated camera pose.</p>
<div class="image">
<img src="algo_block_diagram.png" alt="algo_block_diagram.png"/>
</div>
<hr/>
<h1><a class="anchor" id="autotoc_md149"></a>
Deep learning based descriptor (DKAZE)</h1>
<p>Key-point Descriptor plays critical role in ensuring high accuracy during Visual localization process. However descriptor computation can put quite a load on compute resources like CPU/DSPs. To address this issue we adopt Deep Learning based approach. We train deep neural net (DNN) to learn hand computed feature descriptor like KAZE in a supervised manner. We refer such descriptor as DKAZE. As DKAZE uses very similar network architecture as typical semantics segmentation networks, in actual deployment DKAZE can be considered as an additional decoder in multitask network (single encoder + multiple decoders topology).</p>
<div class="image">
<img src="dkaze.jpg" alt="dkaze.jpg"/>
</div>
<hr/>
 <div id="dataset"></div><h1><a class="anchor" id="autotoc_md151"></a>
Dataset creation using Carla Simulator</h1>
<p>Along with the out-of-box example, dataset generated using Carla Simulator 0.9.9.4 (<a href="https://carla.org/">https://carla.org/</a>) is provided. Dataset contains the following,</p>
<ol type="1">
<li>Camera images</li>
<li>Ground truth position (6 Degree Of Freedom)</li>
<li>Ground truth depth</li>
<li>Camera intrinsic (Focal length and camera center)</li>
</ol>
<p>The snapshot of one of the captured images is shown below.</p>
<div class="image">
<img src="0000000304.png" alt="0000000304.png"/>
</div>
<p>We also make Python script to capture dataset using Carla available on request.</p>
<hr/>
<h1><a class="anchor" id="autotoc_md153"></a>
Localization Accuracy</h1>
<p>The following is the X-Z plot of the localization position estimates of vehicles with ground truth vehicle positions. It is evident that visual localization output is very close to ground truth locations. Also it is able to handle sharp turn without any issue.</p>
<div class="image">
<img src="map_X_Z_lat_lon.jpg" alt="map_X_Z_lat_lon.jpg"/>
</div>
<p>Here is the objective comparison with GT positions, </p><table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadNone">use-case  </th><th class="markdownTableHeadNone">Average error   </th></tr>
<tr class="markdownTableBody" class="markdownTableRowOdd">
<td class="markdownTableBodyNone">10 fps  </td><td class="markdownTableBodyNone">16.3 cm   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowEven">
<td class="markdownTableBodyNone">30 fps  </td><td class="markdownTableBodyNone">10.6 cm   </td></tr>
</table>
<hr/>
 <div id="DataFlow"></div><h1><a class="anchor" id="autotoc_md155"></a>
Visual Localization Data Flow</h1>
<p>The following conceptual block diagram indicates where various components of visualization location pipeline get mapped to various HW resources available on SOC.</p>
<div class="image">
<img src="resources_block_diagram.png" alt="resources_block_diagram.png"/>
</div>
<p>In this demo, we read file based NV12 inputs of resolution 768x384 from SD card and pass it to a resize MSC. As the input resolution is matching with resolution required by DKAZE network, the input and output resolution of resizer stage is the same. This is then passed to C66x_1 for pre-processing where NV12 image is converted to RGB/BGR planar image along with necessary padding as required by TIDL. The DKAZE features are then obtained from TIDL running on C7x and is passed to C66x_2 for pose calculation. The result is then sent to another OpenVx node running on the same C66x_2 core for visualization where the position of the car is indicated by a blue pixel. The intensity of the pixel changes as the ego vehicle moves. This information is overlayed on the top view of the map and passed to SW mosaic node for overlay with front camera image on a 1920x1080 NV12 plane. This plane is submitted to DSS for display.</p>
<p>Note: On J721S2, the algorithms running on C66_1 and C66_2 have been recompiled and are running on the standalone C7X.</p>
<div class="image">
<img src="app_tidl_vl_data_flow.png" alt="app_tidl_vl_data_flow.png"/>
</div>
 <hr/>
<h1><a class="anchor" id="autotoc_md157"></a>
Sparse 3D Map</h1>
<p>During localization deployment per Kilometer MAP size is one of the critical factors to ensure real time operations. We adopt sparse 3D format for the Map storage, which does not need camera images, resulting in more compact MAP size. With out-of-box visual localization example, the off-line created sparse 3D map is provided. One can create her own off-line map in the following format,</p>
<div class="fragment"><div class="line">#MapParams: MapVersion  FeatureType FeatureParams_1 FeatureParams_2 MapRange_xmin   MapRange_xmax   MapRange_ymin   MapRange_ymax   MapRange_zmin   MapRange_zmax   MapVoxelSize_x  MapVoxelSize_y  MapVoxelSize_z  MapRealDimensions_x MapRealDimensions_y MapRealDimensions_z MapVoxelDimensions_x    MapVoxelDimensions_y    MapVoxelDimensions_z    ImageScalingFactor_x,   ImageScalingFactor_y</div><div class="line"></div><div class="line">#List of voxels</div><div class="line">#[X,Y,Z,P]: X_index Y_index Z_index Number_Points_in_Voxel</div><div class="line">#pt-1</div><div class="line">location_x  location_y  location_z  Feature-descriptor-64-elements</div><div class="line">#pt-2</div><div class="line">location_x  location_y  location_z  Feature-descriptor-64-elements</div><div class="line">#pt-3</div><div class="line">location_x  location_y  location_z  Feature-descriptor-64-elements</div><div class="line">.......</div><div class="line">.......</div><div class="line">.......</div><div class="line">#pt-Number_Points_in_Voxel</div><div class="line">location_x  location_y  location_z  Feature-descriptor-64-elements</div></div><!-- fragment --><p>An example map file, </p><div class="fragment"><div class="line">#MapParams: 2 4 3 4 -250 250 -5 5 -250 250 3 3 3 501 11 501 167 4 167 1. 1.</div><div class="line"></div><div class="line"></div><div class="line">#[X,Y,Z,P]: 0 0 0 0</div><div class="line"></div><div class="line">#[X,Y,Z,P]: 0 0 1 0</div><div class="line"></div><div class="line">#[X,Y,Z,P]: 0 0 2 0</div><div class="line"></div><div class="line">#[X,Y,Z,P]: 1 3 50 2</div><div class="line"></div><div class="line">-247.407654 1.428528 -100.674500 122. 123. 41. 45. 121. 118. 59. 75. 128. 119. 61. 74. 123. 122. 43. 42. 127. 128. 69. 68. 130. 148. 78. 117. 126. 152. 83. 114. 127. 135. 69. 64. 135. 122. 72. 48. 138. 132. 85. 84. 111. 128. 88. 86. 116. 124. 72. 55. 130. 120. 42. 32. 131. 107. 56. 62. 119. 108. 59. 58. 121.</div><div class="line">-247.216385 1.640076 -101.193047 122. 123. 44. 46. 127. 118. 57. 77. 128. 116. 61. 75. 129. 123. 39. 46. 126. 133. 68. 76. 134. 156. 76. 140. 121. 154. 78. 132. 128. 133. 61. 75. 135. 120. 70. 54. 144. 109. 88. 93. 111. 106. 92. 92. 124. 118. 66. 56. 130. 123. 41. 38. 131. 119. 55. 67. 124. 122. 58. 69. 126.</div><div class="line">#[X,Y,Z,P]: 1 3 51 0</div><div class="line"></div><div class="line">#[X,Y,Z,P]: 1 3 52 2</div><div class="line"></div><div class="line">-247.520508 1.129715 -95.023788 125. 123. 14. 15. 128. 118. 20. 22. 126. 119. 23. 24. 123. 122. 15. 11. 127. 141. 23. 52. 126. 150. 34. 86. 130. 151. 34. 89. 119. 140. 24. 54. 135. 167. 28. 105. 133. 190. 31. 198. 122. 187. 34. 199. 121. 161. 27. 112. 128. 133. 13. 45. 128. 131. 8. 83. 124. 126. 9. 82. 126.</div><div class="line">-248.440674 1.048920 -94.369522 124. 123. 9. 16. 127. 125. 18. 22. 128. 119. 23. 24. 124. 130. 16. 16. 124. 143. 19. 44. 120. 147. 27. 71. 137. 144. 33. 73. 128. 142. 26. 46. 134. 167. 19. 109. 126. 209. 23. 200. 129. 206. 25. 198. 124. 161. 19. 106. 129. 124. 12. 52. 129. 118. 9. 91. 124. 116. 9. 88. 126.</div><div class="line"></div><div class="line"></div><div class="line">.....................</div><div class="line">.....................</div><div class="line">.....................</div></div><!-- fragment --> <hr/>
<h1><a class="anchor" id="autotoc_md159"></a>
Optimized Building Blocks for your own Visual localization pipeline</h1>
<p>As an out-of-box example end-to-end optimized visual localization pipeline SW is provided. However one can build her own pipeline by either replacing few compute blocks of her own choice or create visual localization pipeline from scratch but accelerate compute heavy blocks by utilizing available optimized blocks. The following are the optimized compute blocks are available as part of TIADALG component package.</p>
<p><b> 1. Two Way Descriptor Matching -</b></p>
<p>Name of the API: tiadalg_select_top_feature()</p>
<p>This API can be used to carry out two way matching between 2 sets of descriptors. Typically descriptors sample form the sparse 3D map and descriptors computed using current frame need to be matched to find N best matching pairs. This API does two directional matching to avoid wrong matches resulting from feature less regions like flat surfaces. It supports unsigned 16 and unsigned 8 bit descriptors datatype. Length of descriptor is configurable. The current example exercises 8 bit unsigned type descriptor with 64 elements per descriptor.</p>
<p><b> 2. Sparse Up-sampling -</b></p>
<p>Name of the API: tiadalg_sparse_upsampling()</p>
<p>DKAZE module generates the descriptors at 1/4th image resolution for optimal memory usage. However Visual localization pipeline needs it at full resolution. To do up-sampling process in most optimal manner this API does up-sampling only on the sparse points selected by key point detector using nearest neighbor resizing. It also applies 7x7 filtering on the resized data.</p>
<p><b> 3. Recursive NMS -</b></p>
<p>Name of the API: tiadalg_image_recursive_nms()</p>
<p>DKAZE module produces key point score plane buffer at original image resolution. It does scores plane buffer thresholding, and then 3x3 NMS is performed on thresholded score buffer. This is helpful getting relevant interest points in the scenario where scores are saturated in clusters. These compute blocks executes on C66x DSP as part of visual localization OpenVX node.</p>
<p><b> 4. Perspective N Point Pose estimation, a.k.a. SolvePnP -</b></p>
<p>Name of the API: tiadalg_solve_pnp()</p>
<p>After establishing 2D-3D correspondences using, two way descriptor matching API shown above, one can use perspective N point API to find 6 DOF camera pose.</p>
<p>More details about these API interfaces and compute performance details are provided in user guide and data sheet of TIADALG component.</p>
<hr/>
<h1><a class="anchor" id="autotoc_md161"></a>
Sample Output</h1>
<p>The following picture shows trajectory of estimated positions generated using visual localization (thick blue line) on the top-view image. The thin white line indicates ground truth trajectory.</p>
<div class="image">
<img src="app_tidl_vl_output.png" alt="app_tidl_vl_output.png"/>
</div>
 <hr/>
 </div></div><!-- contents -->
</div><!-- doc-content -->
<!-- HTML footer for doxygen 1.8.11-->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.14 </li>
  </ul>
</div>
</body>
</html>
